{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>jurisdiction</th>\n",
       "      <th>opinion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1441798</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>['appeal', 'decree', 'chancery', 'court', 'can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1898932</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>['dissent', 'issue', 'fact', 'exactly', 'state...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1727410</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>['file', 'present', 'suit', 'seeking’', 'alleg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>236201</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>['find', 'guilty', 'capital', 'felony', 'murde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6935606</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>['concurring', 'dissent', 'concur', 'majority'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   case_id jurisdiction                                            opinion\n",
       "0  1441798     Arkansas  ['appeal', 'decree', 'chancery', 'court', 'can...\n",
       "1  1898932     Arkansas  ['dissent', 'issue', 'fact', 'exactly', 'state...\n",
       "2  1727410     Arkansas  ['file', 'present', 'suit', 'seeking’', 'alleg...\n",
       "3   236201     Arkansas  ['find', 'guilty', 'capital', 'felony', 'murde...\n",
       "4  6935606     Arkansas  ['concurring', 'dissent', 'concur', 'majority'..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a baseline model using basic preprocessing pipeline.\n",
    "# Dataset is roughly 50000 opinions pulled randomly; 20000 from each of AK, IL and remainder from NM\n",
    "# The opinions were chosen from the list of those which either cite another case in the corpus, or which\n",
    "# are cited by another case in the corpus\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# NOTE: for preprocessing algorithm, see the \"Preprocessing\" section of MVP.py\n",
    "\n",
    "data = pd.read_csv('../data_uncompressed/random_cases1_processed.csv')\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_list_col(df, col_to_parse):\n",
    "    df.loc[:, col_to_parse] = df[col_to_parse].apply(lambda x: x.strip('[]').split(','))\n",
    "    df.loc[:, col_to_parse] = df[col_to_parse].apply(lambda x: [t.strip().strip(\"'\") for t in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_list_col(data, 'opinion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data to ensure jurisdictions are mixed properly.\n",
    "\n",
    "seed = 9\n",
    "data = data.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test sets\n",
    "\n",
    "split = int(0.8*data.shape[0])\n",
    "train_ops = data.loc[:split, 'opinion']\n",
    "test_ops = data.loc[split:, 'opinion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "op_dictionary = Dictionary(train_ops.to_list())\n",
    "train_op_corpus = [op_dictionary.doc2bow(op) for op in train_ops.to_list()]\n",
    "test_op_corpus = [op_dictionary.doc2bow(op) for op in test_ops.to_list()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain largest connected component of citation graph.\n",
    "# I will analyze this component further in another notebook.\n",
    "\n",
    "G = nx.read_gpickle('../citation_graph.gpickle')\n",
    "big_subgraph = nx.subgraph(G, list(nx.connected_components(G))[0])\n",
    "G = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS FOR PERPLEXITY, COHERENCE, AND WORDCLOUDS\n",
    "def get_perplexity(model, corpus):\n",
    "    return 2**(-model.log_perplexity(corpus))\n",
    "\n",
    "def get_coherence(model, texts, dictionary):\n",
    "    # texts should be lists of terms, not the BoW representation\n",
    "    coherence_model = CoherenceModel(model=model, texts=texts, \n",
    "                                 dictionary=dictionary, coherence='c_v')\n",
    "    return coherence_model.get_coherence()\n",
    "\n",
    "def get_wordclouds(model, num_words=250, save_file=None, num_topics=10):\n",
    "    for i, topic in  enumerate(model.show_topics(num_topics=num_topics, num_words=num_words, formatted=False)):\n",
    "        topic_dict = {w:v for (w,v) in topic[1]}\n",
    "\n",
    "        wordcloud = WordCloud(width = 800, height = 800, \n",
    "                        background_color ='white',\n",
    "                        min_font_size = 10).generate_from_frequencies(topic_dict) \n",
    "\n",
    "        # plot the WordCloud image                        \n",
    "        plt.figure(figsize = (8, 8), facecolor = None) \n",
    "        plt.imshow(wordcloud) \n",
    "        plt.axis(\"off\") \n",
    "        plt.tight_layout(pad = 0) \n",
    "        if save_file:\n",
    "            path = os.path.join(save_file, f'topic_{i+1}.png')\n",
    "            plt.savefig(path)\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS FOR CITATION-GRAPH KNN VALIDATION\n",
    "\n",
    "def unpack_topics(df, num_topics):\n",
    "    new_df = pd.DataFrame(columns=['case_id']+[f'topic_{i}' for i in range(num_topics)])\n",
    "    for i, row in df.iterrows():\n",
    "        new_row = {}\n",
    "        new_row['case_id'] = row.case_id\n",
    "        topics = row.topic_vector\n",
    "        for t in topics:\n",
    "            topic_num = t[0]\n",
    "            topic_val = t[1]\n",
    "            new_row[f'topic_{topic_num}'] = topic_val\n",
    "        new_df = new_df.append(new_row, ignore_index=True)\n",
    "    new_df = new_df.fillna(0)\n",
    "    new_df['case_id'] = new_df['case_id'].apply(lambda x: int(x))\n",
    "    return new_df\n",
    "\n",
    "def get_nearest_neighbors(df, n_neighbors, nn_model):\n",
    "    knearest = nn_model.kneighbors(n_neighbors=n_neighbors, return_distance=False)\n",
    "    for k in range(n_neighbors):\n",
    "        df[f'nn_{k}'] = [df.case_id[x[k]] for x in knearest]\n",
    "        \n",
    "def edge_length(row, k, graph):\n",
    "    return nx.shortest_path_length(graph, row['case_id'], row[f'nn_{k}'])\n",
    "\n",
    "def get_edge_lengths(df, n_neighbors, graph):\n",
    "    for k in range(n_neighbors):\n",
    "        df[f'cite_distance_{k}'] = df.apply(edge_length, k=k, graph=graph, axis=1)\n",
    "        \n",
    "def get_min_cite_dist(row, n_neighbors):\n",
    "    return int(row[[f'cite_distance_{k}' for k in range(n_neighbors)]].min())\n",
    "\n",
    "def get_mean_cite_dist(row, n_neighbors):\n",
    "    return row[[f'cite_distance_{k}' for k in range(n_neighbors)]].mean()\n",
    "\n",
    "def get_max_cite_dist(row, n_neighbors):\n",
    "    return int(row[[f'cite_distance_{k}' for k in range(n_neighbors)]].max())\n",
    "\n",
    "def knn_citation_validation(test_ids, lda_model, test_corpus, graph, n_neighbors):\n",
    "    test_data = pd.DataFrame(test_ids, columns=['case_id'])\n",
    "    test_data['topic_vector'] = [lda_model[op] for op in test_corpus]\n",
    "    nodes = list(graph.nodes)\n",
    "    test_data = test_data.loc[test_data.case_id.isin(nodes),:]\n",
    "    nodes = None\n",
    "    test_data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    test_data = unpack_topics(test_data, num_topics=15)\n",
    "    \n",
    "    nn = NearestNeighbors()\n",
    "\n",
    "    X = test_data.drop(columns='case_id').values\n",
    "\n",
    "    nn.fit(X)\n",
    "    \n",
    "    get_nearest_neighbors(test_data, n_neighbors, nn)\n",
    "    \n",
    "    get_edge_lengths(test_data, n_neighbors, graph)\n",
    "    \n",
    "    test_data['min_cite_dist'] = test_data.apply(get_min_cite_dist, axis=1, n_neighbors=n_neighbors)\n",
    "    test_data['mean_cite_dist'] = test_data.apply(get_mean_cite_dist, axis=1, n_neighbors=n_neighbors)\n",
    "    test_data['max_cite_dist'] = test_data.apply(get_max_cite_dist, axis=1, n_neighbors=n_neighbors)\n",
    "    \n",
    "    return test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN VALIDATION. THIS WILL TAKE SOME TIME.\n",
    "\n",
    "num_topics = [3, 5, 10, 15, 20]\n",
    "train_perplexity = []\n",
    "test_perplexity = []\n",
    "train_coherence = []\n",
    "test_coherence = []\n",
    "min_cite_dist_mean = []\n",
    "min_cite_dist_sd = []\n",
    "avg_cite_dist_mean = []\n",
    "avg_cite_dist_sd = []\n",
    "max_cite_dist_mean = []\n",
    "max_cite_dist_sd = []\n",
    "\n",
    "test_ids = data.loc[split:, 'case_id'].to_list()\n",
    "data = None\n",
    "\n",
    "wordcloud_header = '../wordclouds/baseline_50k'\n",
    "output_header = '../validation_output'\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for nt in num_topics:\n",
    "    iter_start=time.time()\n",
    "    print(f'Processing model with {nt} topics...')\n",
    "    temp_time= time.time()\n",
    "    lda = LdaModel(train_op_corpus, id2word=op_dictionary, num_topics=nt)\n",
    "    print(f'Model training done. Time: {round(time.time()-temp_time)}')\n",
    "    print('Computing perplexity on train set.')\n",
    "    temp_time= time.time()\n",
    "    train_perplexity.append(get_perplexity(lda, train_op_corpus))\n",
    "    print(f'Done. Time: {round(time.time()-temp_time)}')\n",
    "    print('Computing perplexity on test set.')\n",
    "    temp_time= time.time()\n",
    "    test_perplexity.append(get_perplexity(lda, test_op_corpus))\n",
    "    print(f'Done. Time: {round(time.time()-temp_time)}')\n",
    "    print('Computing coherence on train set.')\n",
    "    temp_time= time.time()\n",
    "    train_coherence.append(get_coherence(lda, train_ops.to_list(), op_dictionary))\n",
    "    print(f'Done. Time: {round(time.time()-temp_time)}')\n",
    "    print('Computing coherence on test set.')\n",
    "    temp_time= time.time()\n",
    "    test_coherence.append(get_coherence(lda, test_ops.to_list(), op_dictionary))\n",
    "    print(f'Done. Time: {round(time.time()-temp_time)}')\n",
    "    print('Computing citation graph validation metrics.')\n",
    "    temp_time= time.time()\n",
    "    metric_cols = ['min_cite_dist', 'mean_cite_dist', 'max_cite_dist']\n",
    "    citation_dist_results = knn_citation_validation(test_ids, lda, test_op_corpus, big_subgraph, 5)[metric_cols]\n",
    "    min_cite_dist_mean.append(citation_dist_results.min_cite_dist.mean())\n",
    "    min_cite_dist_sd.append(citation_dist_results.min_cite_dist.std())\n",
    "    avg_cite_dist_mean.append(citation_dist_results.mean_cite_dist.mean())\n",
    "    avg_cite_dist_sd.append(citation_dist_results.mean_cite_dist.std())\n",
    "    max_cite_dist_mean.append(citation_dist_results.max_cite_dist.mean())\n",
    "    max_cite_dist_sd.append(citation_dist_results.max_cite_dist.std())\n",
    "    print(f'Done. Time: {round(time.time()-temp_time)}')\n",
    "    print('Saving wordclouds...')\n",
    "    temp_time= time.time()\n",
    "    os.mkdir(os.path.join(wordcloud_header, f'num_topics_{nt}'))\n",
    "    get_wordclouds(lda, save_file=os.path.join(wordcloud_header, f'num_topics_{nt}'), num_topics=nt)\n",
    "    print(f'Done. Time: {round(time.time()-temp_time)}')\n",
    "    print(f'Done with full iteration. TOTAL TIME: {round(time.time()-iter_start)}')\n",
    "    print('######################################')\n",
    "print(f'FINISHED. TOTAL TIME ELAPSED: {time.time()-start}')\n",
    "   \n",
    "results_df = pd.DataFrame({'num_topics': num_topics,\n",
    "                          'train_perplexity': train_perplexity,\n",
    "                          'test_perplexity': test_perplexity,\n",
    "                          'train_coherence': train_coherence,\n",
    "                          'test_coherence': test_coherence,\n",
    "                          'min_cite_dist_mean': min_cite_dist_mean,\n",
    "                          'min_cite_dist_sd': min_cite_dist_sd,\n",
    "                          'avg_cite_dist_mean': avg_cite_dist_mean,\n",
    "                          'avg_cite_dist_sd': avg_cite_dist_sd,\n",
    "                          'max_cite_dist_mean': max_cite_dist_mean,\n",
    "                          'max_cite_dist_sd': max_cite_dist_sd})\n",
    "\n",
    "results_df.to_csv(os.path.join(output_header, 'baseline_50k.csv'), index=False)\n",
    "\n",
    "train_ops= None\n",
    "test_ops = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_topics</th>\n",
       "      <th>train_perplexity</th>\n",
       "      <th>test_perplexity</th>\n",
       "      <th>train_coherence</th>\n",
       "      <th>test_coherence</th>\n",
       "      <th>min_cite_dist_mean</th>\n",
       "      <th>min_cite_dist_sd</th>\n",
       "      <th>avg_cite_dist_mean</th>\n",
       "      <th>avg_cite_dist_sd</th>\n",
       "      <th>max_cite_dist_mean</th>\n",
       "      <th>max_cite_dist_sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>173.355861</td>\n",
       "      <td>179.961553</td>\n",
       "      <td>0.330991</td>\n",
       "      <td>0.330970</td>\n",
       "      <td>4.585939</td>\n",
       "      <td>1.183006</td>\n",
       "      <td>5.953500</td>\n",
       "      <td>0.912301</td>\n",
       "      <td>7.240468</td>\n",
       "      <td>1.049598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>165.945705</td>\n",
       "      <td>174.702039</td>\n",
       "      <td>0.391956</td>\n",
       "      <td>0.390354</td>\n",
       "      <td>4.378051</td>\n",
       "      <td>1.244662</td>\n",
       "      <td>5.788320</td>\n",
       "      <td>0.942322</td>\n",
       "      <td>7.110248</td>\n",
       "      <td>1.075037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>156.449843</td>\n",
       "      <td>167.399101</td>\n",
       "      <td>0.409653</td>\n",
       "      <td>0.408722</td>\n",
       "      <td>4.140811</td>\n",
       "      <td>1.337198</td>\n",
       "      <td>5.600908</td>\n",
       "      <td>1.022035</td>\n",
       "      <td>6.969941</td>\n",
       "      <td>1.126026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>152.003282</td>\n",
       "      <td>163.717241</td>\n",
       "      <td>0.449244</td>\n",
       "      <td>0.448440</td>\n",
       "      <td>4.004337</td>\n",
       "      <td>1.361399</td>\n",
       "      <td>5.466633</td>\n",
       "      <td>1.078904</td>\n",
       "      <td>6.857777</td>\n",
       "      <td>1.178315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>149.514835</td>\n",
       "      <td>163.788020</td>\n",
       "      <td>0.484945</td>\n",
       "      <td>0.484455</td>\n",
       "      <td>3.938370</td>\n",
       "      <td>1.399130</td>\n",
       "      <td>5.410046</td>\n",
       "      <td>1.131191</td>\n",
       "      <td>6.804216</td>\n",
       "      <td>1.234650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_topics  train_perplexity  test_perplexity  train_coherence  \\\n",
       "0           3        173.355861       179.961553         0.330991   \n",
       "1           5        165.945705       174.702039         0.391956   \n",
       "2          10        156.449843       167.399101         0.409653   \n",
       "3          15        152.003282       163.717241         0.449244   \n",
       "4          20        149.514835       163.788020         0.484945   \n",
       "\n",
       "   test_coherence  min_cite_dist_mean  min_cite_dist_sd  avg_cite_dist_mean  \\\n",
       "0        0.330970            4.585939          1.183006            5.953500   \n",
       "1        0.390354            4.378051          1.244662            5.788320   \n",
       "2        0.408722            4.140811          1.337198            5.600908   \n",
       "3        0.448440            4.004337          1.361399            5.466633   \n",
       "4        0.484455            3.938370          1.399130            5.410046   \n",
       "\n",
       "   avg_cite_dist_sd  max_cite_dist_mean  max_cite_dist_sd  \n",
       "0          0.912301            7.240468          1.049598  \n",
       "1          0.942322            7.110248          1.075037  \n",
       "2          1.022035            6.969941          1.126026  \n",
       "3          1.078904            6.857777          1.178315  \n",
       "4          1.131191            6.804216          1.234650  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
