{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook implements the preprocessing routine which prunes the list of basic legal stop words\n",
    "in addition to the baseline preprocessing tasks.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import spacy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "raw_data_header = '/Users/jhamer90811/Documents/Insight/legal_topic_modeling/data_uncompressed/raw_data_12k'\n",
    "\n",
    "# Note: due to time constraints, I am only preprocessing the 'cases_IL_after1950_12k' dataset\n",
    "\n",
    "# datasets = ['random_cases2', 'cases_after1950_12k', 'cases_IL_12k', 'cases_IL_after1950_12k']\n",
    "\n",
    "datasets = ['cases_IL_after1950_12k']\n",
    "\n",
    "basic_legal_stopwords = {'a.',\n",
    " 'a.2d',\n",
    " 'a.3d',\n",
    " 'appeal',\n",
    " 'appellant',\n",
    " 'appellee',\n",
    " 'case',\n",
    " 'cir',\n",
    " 'court',\n",
    " 'defendant',\n",
    " 'f. supp.',\n",
    " 'f.supp.',\n",
    " 'f.supp.2d',\n",
    " 'f.supp.3d',\n",
    " 'fact',\n",
    " 'find',\n",
    " 'hold',\n",
    " 'judgment',\n",
    " 'n.e.',\n",
    " 'n.e.2d',\n",
    " 'opinion',\n",
    " 'order',\n",
    " 'p.',\n",
    " 'p.2d',\n",
    " 'p.3d',\n",
    " 'plaintiff',\n",
    " 'question',\n",
    " 's.e.',\n",
    " 's.e.2d',\n",
    " 's.e.3d',\n",
    " 's.w.',\n",
    " 's.w.2d',\n",
    " 's.w.3d.',\n",
    " 'see',\n",
    " 'so.',\n",
    " 'so.2d',\n",
    " 'state',\n",
    " 'time',\n",
    " 'trial'}\n",
    "\n",
    "processed_data_header = '/Users/jhamer90811/Documents/Insight/legal_topic_modeling/data_uncompressed/basic_stopwords_12k'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning preprocessing of opinions for cases_IL_after1950_12k...\n",
      "Done with 1000 opinions.\n",
      "Time elapsed: 209\n",
      "#######################################\n",
      "Done with 2000 opinions.\n",
      "Time elapsed: 448\n",
      "#######################################\n",
      "Done with 3000 opinions.\n",
      "Time elapsed: 684\n",
      "#######################################\n",
      "Done with 4000 opinions.\n",
      "Time elapsed: 914\n",
      "#######################################\n",
      "Done with 5000 opinions.\n",
      "Time elapsed: 1152\n",
      "#######################################\n",
      "Done with 6000 opinions.\n",
      "Time elapsed: 1395\n",
      "#######################################\n",
      "Done with 7000 opinions.\n",
      "Time elapsed: 1631\n",
      "#######################################\n",
      "Done with 8000 opinions.\n",
      "Time elapsed: 1875\n",
      "#######################################\n",
      "Done with 9000 opinions.\n",
      "Time elapsed: 2108\n",
      "#######################################\n",
      "Done with 10000 opinions.\n",
      "Time elapsed: 2345\n",
      "#######################################\n",
      "Done with 11000 opinions.\n",
      "Time elapsed: 2574\n",
      "#######################################\n",
      "Done with 12000 opinions.\n",
      "Time elapsed: 2802\n",
      "#######################################\n",
      "Done preprocessing. Took 2802 seconds.\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "for dataset in datasets:\n",
    "    data_raw = pd.read_csv(os.path.join(raw_data_header, dataset + '_raw.csv'))\n",
    "    start = time.time()\n",
    "    opinions = []\n",
    "    print(f'Beginning preprocessing of opinions for {dataset}...')\n",
    "    for i, row in data_raw.iterrows():\n",
    "        case_id = row.case_id\n",
    "        juris = row.jurisdiction\n",
    "        text = row.opinion\n",
    "        court = row.court_name\n",
    "        if dataset=='random_cases2':\n",
    "            decision_year = pd.to_datetime(row.decision_date).year\n",
    "        else:\n",
    "            decision_year = row.decision_year\n",
    "        doc = nlp(text)\n",
    "        doc = [t.lemma_ for t in doc if (not t.is_stop) and\\\n",
    "                                        (t.lemma_ not in basic_legal_stopwords) and\\\n",
    "                                        (t.pos_ in ['NOUN', 'ADJ', 'VERB', 'ADV'])]\n",
    "        opinions.append((case_id, juris, court, decision_year, doc))\n",
    "        if (i+1)%1000==0:\n",
    "            print(f'Done with {i+1} opinions.')\n",
    "            print(f'Time elapsed: {round(time.time()-start)}')\n",
    "            print('#######################################')\n",
    "    print(f'Done preprocessing. Took {round(time.time()-start)} seconds.')\n",
    "\n",
    "    data_raw = None\n",
    "\n",
    "    df = pd.DataFrame(opinions, columns=['case_id', 'jurisdiction', 'court', 'decision_year', 'opinion'])\n",
    "    opinions = None\n",
    "    df.to_csv(os.path.join(processed_data_header, dataset + '_processed.csv'), index=False)\n",
    "\n",
    "    df = None\n",
    "    \n",
    "nlp = None  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
