{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this notebook we train, validate, and visualize different topic models for the basic_stopwords\n",
    "preprocessing routine. We use the MALLET LDA model instead of standard LDA.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "processed_data_header = '/Users/jhamer90811/Documents/Insight/legal_topic_modeling/data_uncompressed/basic_stopwords_12k'\n",
    "\n",
    "# Note: due to time constraints, I am only preprocessing the 'cases_IL_after1950_12k' dataset\n",
    "\n",
    "# datasets = ['random_cases2', 'cases_after1950_12k', 'cases_IL_12k', 'cases_IL_after1950_12k']\n",
    "\n",
    "datasets = ['cases_IL_after1950_12k']\n",
    "\n",
    "graph_path = '/Users/jhamer90811/Documents/Insight/legal_topic_modeling/citation_graph.gpickle'\n",
    "\n",
    "output_header = '/Users/jhamer90811/Documents/Insight/legal_topic_modeling/validation_output/basic_stopwords_mallet_12k'\n",
    "\n",
    "wordcloud_header_top = '/Users/jhamer90811/Documents/Insight/legal_topic_modeling/wordclouds/basic_stopwords_mallet_12k'\n",
    "\n",
    "path_to_mallet_bin = '/Users/jhamer90811/Documents/Insight/Mallet/bin'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_list_col(df, col_to_parse):\n",
    "    df.loc[:, col_to_parse] = df[col_to_parse].apply(lambda x: x.strip('[]').split(','))\n",
    "    df.loc[:, col_to_parse] = df[col_to_parse].apply(lambda x: [t.strip().strip(\"'\") for t in x])\n",
    "    \n",
    "# HELPER FUNCTIONS FOR PERPLEXITY, COHERENCE, AND WORDCLOUDS\n",
    "def get_perplexity(model, corpus):\n",
    "    return 2**(-model.log_perplexity(corpus))\n",
    "\n",
    "def get_coherence(model, texts, dictionary):\n",
    "    # texts should be lists of terms, not the BoW representation\n",
    "    coherence_model = CoherenceModel(model=model, texts=texts, \n",
    "                                 dictionary=dictionary, coherence='c_v')\n",
    "    return coherence_model.get_coherence()\n",
    "\n",
    "def get_wordclouds(model, num_words=250, save_file=None, num_topics=10):\n",
    "    for i, topic in  enumerate(model.show_topics(num_topics=num_topics, num_words=num_words, formatted=False)):\n",
    "        topic_dict = {w:v for (w,v) in topic[1]}\n",
    "\n",
    "        wordcloud = WordCloud(width = 800, height = 800, \n",
    "                        background_color ='white',\n",
    "                        min_font_size = 10).generate_from_frequencies(topic_dict) \n",
    "\n",
    "        # plot the WordCloud image                        \n",
    "        plt.figure(figsize = (8, 8), facecolor = None) \n",
    "        plt.imshow(wordcloud) \n",
    "        plt.axis(\"off\") \n",
    "        plt.tight_layout(pad = 0) \n",
    "        if save_file:\n",
    "            path = os.path.join(save_file, f'topic_{i+1}.png')\n",
    "            plt.savefig(path)\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "# HELPER FUNCTIONS FOR CITATION-GRAPH KNN VALIDATION\n",
    "\n",
    "def unpack_topics(df, num_topics):\n",
    "    new_df = pd.DataFrame(columns=['case_id']+[f'topic_{i}' for i in range(num_topics)])\n",
    "    for i, row in df.iterrows():\n",
    "        new_row = {}\n",
    "        new_row['case_id'] = row.case_id\n",
    "        topics = row.topic_vector\n",
    "        for t in topics:\n",
    "            topic_num = t[0]\n",
    "            topic_val = t[1]\n",
    "            new_row[f'topic_{topic_num}'] = topic_val\n",
    "        new_df = new_df.append(new_row, ignore_index=True)\n",
    "    new_df = new_df.fillna(0)\n",
    "    new_df['case_id'] = new_df['case_id'].apply(lambda x: int(x))\n",
    "    return new_df\n",
    "\n",
    "def get_nearest_neighbors(df, n_neighbors, nn_model):\n",
    "    knearest = nn_model.kneighbors(n_neighbors=n_neighbors, return_distance=False)\n",
    "    for k in range(n_neighbors):\n",
    "        df[f'nn_{k}'] = [df.case_id[x[k]] for x in knearest]\n",
    "        \n",
    "def edge_length(row, k, graph):\n",
    "    return nx.shortest_path_length(graph, row['case_id'], row[f'nn_{k}'])\n",
    "\n",
    "def get_edge_lengths(df, n_neighbors, graph):\n",
    "    for k in range(n_neighbors):\n",
    "        df[f'cite_distance_{k}'] = df.apply(edge_length, k=k, graph=graph, axis=1)\n",
    "        \n",
    "def get_min_cite_dist(row, n_neighbors):\n",
    "    return int(row[[f'cite_distance_{k}' for k in range(n_neighbors)]].min())\n",
    "\n",
    "def get_mean_cite_dist(row, n_neighbors):\n",
    "    return row[[f'cite_distance_{k}' for k in range(n_neighbors)]].mean()\n",
    "\n",
    "def get_max_cite_dist(row, n_neighbors):\n",
    "    return int(row[[f'cite_distance_{k}' for k in range(n_neighbors)]].max())\n",
    "\n",
    "def knn_citation_validation(test_ids, lda_model, test_corpus, graph, n_neighbors):\n",
    "    test_data = pd.DataFrame(test_ids, columns=['case_id'])\n",
    "    test_data['topic_vector'] = [lda_model[op] for op in test_corpus]\n",
    "    nodes = list(graph.nodes)\n",
    "    test_data = test_data.loc[test_data.case_id.isin(nodes),:]\n",
    "    nodes = None\n",
    "    test_data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    test_data = unpack_topics(test_data, num_topics=15)\n",
    "    \n",
    "    nn = NearestNeighbors()\n",
    "\n",
    "    X = test_data.drop(columns='case_id').values\n",
    "\n",
    "    nn.fit(X)\n",
    "    \n",
    "    get_nearest_neighbors(test_data, n_neighbors, nn)\n",
    "    \n",
    "    get_edge_lengths(test_data, n_neighbors, graph)\n",
    "    \n",
    "    test_data['min_cite_dist'] = test_data.apply(get_min_cite_dist, axis=1, n_neighbors=n_neighbors)\n",
    "    test_data['mean_cite_dist'] = test_data.apply(get_mean_cite_dist, axis=1, n_neighbors=n_neighbors)\n",
    "    test_data['max_cite_dist'] = test_data.apply(get_max_cite_dist, axis=1, n_neighbors=n_neighbors)\n",
    "    \n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain largest connected component of citation graph and other static variables.\n",
    "\n",
    "G = nx.read_gpickle(graph_path)\n",
    "big_subgraph = nx.subgraph(G, list(nx.connected_components(G))[0])\n",
    "G = None\n",
    "seed = 9\n",
    "num_topics = [5, 8, 10, 12, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGINNING VALIDATION OF cases_IL_after1950_12k...\n",
      "Processing model with 5 topics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/insight/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '/Users/jhamer90811/Documents/Insight/Mallet/bin import-file --preserve-case --keep-sequence --remove-stopwords --token-regex \"\\S+\" --input /var/folders/b7/q6hr0ynd37vcy9sp4v000l2h0000gn/T/58e405_corpus.txt --output /var/folders/b7/q6hr0ynd37vcy9sp4v000l2h0000gn/T/58e405_corpus.mallet' returned non-zero exit status 126.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1dc767def31a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Processing model with {nt} topics...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mtemp_time\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLdaMallet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_mallet_bin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_op_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop_dictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Model training done. Time: {round(time.time()-temp_time)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Computing coherence on train set.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/insight/lib/python3.6/site-packages/gensim/models/wrappers/ldamallet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mallet_path, corpus, num_topics, alpha, id2word, workers, prefix, optimize_interval, iterations, topic_threshold)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfinferencer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/insight/lib/python3.6/site-packages/gensim/models/wrappers/ldamallet.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, corpus)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \"\"\"\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmallet_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' train-topics --input %s --num-topics %s  --alpha %s --optimize-interval %s '\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0;34m'--num-threads %s --output-state %s --output-doc-topics %s --output-topic-keys %s '\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/insight/lib/python3.6/site-packages/gensim/models/wrappers/ldamallet.py\u001b[0m in \u001b[0;36mconvert_input\u001b[0;34m(self, corpus, infer, serialize_corpus)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcmd\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfcorpustxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfcorpusmallet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"converting temporary corpus to MALLET format with %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/insight/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(stdout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m   1804\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1805\u001b[0m             \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1806\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1807\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1808\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '/Users/jhamer90811/Documents/Insight/Mallet/bin import-file --preserve-case --keep-sequence --remove-stopwords --token-regex \"\\S+\" --input /var/folders/b7/q6hr0ynd37vcy9sp4v000l2h0000gn/T/58e405_corpus.txt --output /var/folders/b7/q6hr0ynd37vcy9sp4v000l2h0000gn/T/58e405_corpus.mallet' returned non-zero exit status 126."
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(f'BEGINNING VALIDATION OF {dataset}...')\n",
    "    data = pd.read_csv(os.path.join(processed_data_header, dataset + '_processed.csv'))\n",
    "    parse_list_col(data, 'opinion')\n",
    "    \n",
    "    # Shuffle data to ensure jurisdictions are mixed properly.\n",
    "\n",
    "    data = data.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    # Split into train/test sets\n",
    "\n",
    "    split = int(0.8*data.shape[0])\n",
    "    train_ops = data.loc[:split, 'opinion']\n",
    "    test_ops = data.loc[split:, 'opinion']\n",
    "\n",
    "    # Build gensim dictionary\n",
    "\n",
    "    op_dictionary = Dictionary(train_ops.to_list())\n",
    "    train_op_corpus = [op_dictionary.doc2bow(op) for op in train_ops.to_list()]\n",
    "    test_op_corpus = [op_dictionary.doc2bow(op) for op in test_ops.to_list()]\n",
    "    \n",
    "    # BEGIN VALIDATION. THIS WILL TAKE SOME TIME.\n",
    "\n",
    "    train_perplexity = []\n",
    "    test_perplexity = []\n",
    "    train_coherence = []\n",
    "    test_coherence = []\n",
    "    min_cite_dist_mean = []\n",
    "    min_cite_dist_sd = []\n",
    "    avg_cite_dist_mean = []\n",
    "    avg_cite_dist_sd = []\n",
    "    max_cite_dist_mean = []\n",
    "    max_cite_dist_sd = []\n",
    "\n",
    "    test_ids = data.loc[split:, 'case_id'].to_list()\n",
    "    data = None\n",
    "\n",
    "    wordcloud_header = os.path.join(wordcloud_header_top, dataset)\n",
    "    os.mkdir(wordcloud_header)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for nt in num_topics:\n",
    "        iter_start=time.time()\n",
    "        print(f'Processing model with {nt} topics...')\n",
    "        temp_time= time.time()\n",
    "        lda = LdaMallet(path_to_mallet_bin, corpus=train_op_corpus, id2word=op_dictionary, num_topics=nt)\n",
    "        print(f'Model training done. Time: {round(time.time()-temp_time)}')\n",
    "        print('Computing coherence on train set.')\n",
    "        temp_time= time.time()\n",
    "        train_coherence.append(get_coherence(lda, train_ops.to_list(), op_dictionary))\n",
    "        print(f'Done. Time: {round(time.time()-temp_time)}')\n",
    "        print('Computing coherence on test set.')\n",
    "        temp_time= time.time()\n",
    "        test_coherence.append(get_coherence(lda, test_ops.to_list(), op_dictionary))\n",
    "        print(f'Done. Time: {round(time.time()-temp_time)}')\n",
    "        print('Computing citation graph validation metrics.')\n",
    "        temp_time= time.time()\n",
    "        metric_cols = ['min_cite_dist', 'mean_cite_dist', 'max_cite_dist']\n",
    "        citation_dist_results = knn_citation_validation(test_ids, lda, test_op_corpus, big_subgraph, 5)[metric_cols]\n",
    "        min_cite_dist_mean.append(citation_dist_results.min_cite_dist.mean())\n",
    "        min_cite_dist_sd.append(citation_dist_results.min_cite_dist.std())\n",
    "        avg_cite_dist_mean.append(citation_dist_results.mean_cite_dist.mean())\n",
    "        avg_cite_dist_sd.append(citation_dist_results.mean_cite_dist.std())\n",
    "        max_cite_dist_mean.append(citation_dist_results.max_cite_dist.mean())\n",
    "        max_cite_dist_sd.append(citation_dist_results.max_cite_dist.std())\n",
    "        print(f'Done. Time: {round(time.time()-temp_time)}')\n",
    "        print('Saving wordclouds...')\n",
    "        temp_time= time.time()\n",
    "        os.mkdir(os.path.join(wordcloud_header, f'num_topics_{nt}'))\n",
    "        get_wordclouds(lda, save_file=os.path.join(wordcloud_header, f'num_topics_{nt}'), num_topics=nt)\n",
    "        print(f'Done. Time: {round(time.time()-temp_time)}')\n",
    "        print(f'Done with full iteration. TOTAL TIME: {round(time.time()-iter_start)}')\n",
    "        print('######################################')\n",
    "    print(f'FINISHED. TOTAL TIME ELAPSED: {time.time()-start}')\n",
    "\n",
    "    results_df = pd.DataFrame({'num_topics': num_topics,\n",
    "                              'train_coherence': train_coherence,\n",
    "                              'test_coherence': test_coherence,\n",
    "                              'min_cite_dist_mean': min_cite_dist_mean,\n",
    "                              'min_cite_dist_sd': min_cite_dist_sd,\n",
    "                              'avg_cite_dist_mean': avg_cite_dist_mean,\n",
    "                              'avg_cite_dist_sd': avg_cite_dist_sd,\n",
    "                              'max_cite_dist_mean': max_cite_dist_mean,\n",
    "                              'max_cite_dist_sd': max_cite_dist_sd})\n",
    "\n",
    "    results_df.to_csv(os.path.join(output_header, dataset + '.csv'), index=False)\n",
    "\n",
    "    train_ops = None\n",
    "    train_op_corpus = None\n",
    "    test_ops = None\n",
    "    test_op_corpus = None\n",
    "    op_dictionary = None\n",
    "    test_ids = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
