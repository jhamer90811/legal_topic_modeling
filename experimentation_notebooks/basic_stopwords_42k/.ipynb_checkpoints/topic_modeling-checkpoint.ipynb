{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this notebook we train, validate, and visualize different topic models for the basic_stopwords\n",
    "preprocessing routine. The chosen dataset consists of all cases appearing in the citation graph\n",
    "whose jurisdiction is Illinois and whose decision dates occured after 1950.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "processed_data_header = '/Users/jhamer90811/Documents/Insight/legal_topic_modeling/data_uncompressed/basic_stopwords_42k'\n",
    "\n",
    "datasets = ['cases_IL_after1950_42k']\n",
    "\n",
    "graph_path = '/Users/jhamer90811/Documents/Insight/legal_topic_modeling/citation_graph.gpickle'\n",
    "\n",
    "output_header = '/Users/jhamer90811/Documents/Insight/legal_topic_modeling/validation_output/basic_stopwords_42k'\n",
    "\n",
    "wordcloud_header_top = '/Users/jhamer90811/Documents/Insight/legal_topic_modeling/wordclouds/basic_stopwords_42k'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_list_col(df, col_to_parse):\n",
    "    df.loc[:, col_to_parse] = df[col_to_parse].apply(lambda x: x.strip('[]').split(','))\n",
    "    df.loc[:, col_to_parse] = df[col_to_parse].apply(lambda x: [t.strip().strip(\"'\") for t in x])\n",
    "    \n",
    "# HELPER FUNCTIONS FOR PERPLEXITY, COHERENCE, AND WORDCLOUDS\n",
    "def get_perplexity(model, corpus):\n",
    "    return 2**(-model.log_perplexity(corpus))\n",
    "\n",
    "def get_coherence(model, texts, dictionary):\n",
    "    # texts should be lists of terms, not the BoW representation\n",
    "    coherence_model = CoherenceModel(model=model, texts=texts, \n",
    "                                 dictionary=dictionary, coherence='c_v')\n",
    "    return coherence_model.get_coherence()\n",
    "\n",
    "def get_wordclouds(model, num_words=250, save_file=None, num_topics=10):\n",
    "    for i, topic in  enumerate(model.show_topics(num_topics=num_topics, num_words=num_words, formatted=False)):\n",
    "        topic_dict = {w:v for (w,v) in topic[1]}\n",
    "\n",
    "        wordcloud = WordCloud(width = 800, height = 800, \n",
    "                        background_color ='white',\n",
    "                        min_font_size = 10).generate_from_frequencies(topic_dict) \n",
    "\n",
    "        # plot the WordCloud image                        \n",
    "        plt.figure(figsize = (8, 8), facecolor = None) \n",
    "        plt.imshow(wordcloud) \n",
    "        plt.axis(\"off\") \n",
    "        plt.tight_layout(pad = 0) \n",
    "        if save_file:\n",
    "            path = os.path.join(save_file, f'topic_{i+1}.png')\n",
    "            plt.savefig(path)\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "# HELPER FUNCTIONS FOR CITATION-GRAPH KNN VALIDATION\n",
    "\n",
    "def unpack_topics(df, num_topics):\n",
    "    new_df = pd.DataFrame(columns=['case_id']+[f'topic_{i}' for i in range(num_topics)])\n",
    "    for i, row in df.iterrows():\n",
    "        new_row = {}\n",
    "        new_row['case_id'] = row.case_id\n",
    "        topics = row.topic_vector\n",
    "        for t in topics:\n",
    "            topic_num = t[0]\n",
    "            topic_val = t[1]\n",
    "            new_row[f'topic_{topic_num}'] = topic_val\n",
    "        new_df = new_df.append(new_row, ignore_index=True)\n",
    "    new_df = new_df.fillna(0)\n",
    "    new_df['case_id'] = new_df['case_id'].apply(lambda x: int(x))\n",
    "    return new_df\n",
    "\n",
    "def get_nearest_neighbors(df, n_neighbors, nn_model):\n",
    "    knearest = nn_model.kneighbors(n_neighbors=n_neighbors, return_distance=False)\n",
    "    for k in range(n_neighbors):\n",
    "        df[f'nn_{k}'] = [df.case_id[x[k]] for x in knearest]\n",
    "        \n",
    "def edge_length(row, k, graph):\n",
    "    return nx.shortest_path_length(graph, row['case_id'], row[f'nn_{k}'])\n",
    "\n",
    "def get_edge_lengths(df, n_neighbors, graph):\n",
    "    for k in range(n_neighbors):\n",
    "        df[f'cite_distance_{k}'] = df.apply(edge_length, k=k, graph=graph, axis=1)\n",
    "        \n",
    "def get_min_cite_dist(row, n_neighbors):\n",
    "    return int(row[[f'cite_distance_{k}' for k in range(n_neighbors)]].min())\n",
    "\n",
    "def get_mean_cite_dist(row, n_neighbors):\n",
    "    return row[[f'cite_distance_{k}' for k in range(n_neighbors)]].mean()\n",
    "\n",
    "def get_max_cite_dist(row, n_neighbors):\n",
    "    return int(row[[f'cite_distance_{k}' for k in range(n_neighbors)]].max())\n",
    "\n",
    "def knn_citation_validation(test_ids, lda_model, test_corpus, graph, n_neighbors):\n",
    "    test_data = pd.DataFrame(test_ids, columns=['case_id'])\n",
    "    test_data['topic_vector'] = [lda_model[op] for op in test_corpus]\n",
    "    nodes = list(graph.nodes)\n",
    "    test_data = test_data.loc[test_data.case_id.isin(nodes),:]\n",
    "    nodes = None\n",
    "    test_data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    test_data = unpack_topics(test_data, num_topics=15)\n",
    "    \n",
    "    nn = NearestNeighbors()\n",
    "\n",
    "    X = test_data.drop(columns='case_id').values\n",
    "\n",
    "    nn.fit(X)\n",
    "    \n",
    "    get_nearest_neighbors(test_data, n_neighbors, nn)\n",
    "    \n",
    "    get_edge_lengths(test_data, n_neighbors, graph)\n",
    "    \n",
    "    test_data['min_cite_dist'] = test_data.apply(get_min_cite_dist, axis=1, n_neighbors=n_neighbors)\n",
    "    test_data['mean_cite_dist'] = test_data.apply(get_mean_cite_dist, axis=1, n_neighbors=n_neighbors)\n",
    "    test_data['max_cite_dist'] = test_data.apply(get_max_cite_dist, axis=1, n_neighbors=n_neighbors)\n",
    "    \n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain largest connected component of citation graph and other static variables.\n",
    "\n",
    "G = nx.read_gpickle(graph_path)\n",
    "big_subgraph = nx.subgraph(G, list(nx.connected_components(G))[0])\n",
    "G = None\n",
    "seed = 9\n",
    "num_topics = [5, 8, 10, 12, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    print(f'BEGINNING VALIDATION OF {dataset}...')\n",
    "    data = pd.read_csv(os.path.join(processed_data_header, dataset + '_processed.csv'))\n",
    "    # Accidentally appended header several times to data set. Correct for this.\n",
    "    data = data.loc[data.case_id!='case_id',:]\n",
    "    data.loc[:,'case_id'] = data.case_id.apply(lambda x: int(x))\n",
    "    parse_list_col(data, 'opinion')\n",
    "    \n",
    "    # Shuffle data to ensure jurisdictions are mixed properly.\n",
    "\n",
    "    data = data.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    # Split into train/test sets\n",
    "\n",
    "    split = int(0.8*data.shape[0])\n",
    "    train_ops = data.loc[:split, 'opinion']\n",
    "    test_ops = data.loc[split:, 'opinion']\n",
    "\n",
    "    # Build gensim dictionary\n",
    "\n",
    "    op_dictionary = Dictionary(train_ops.to_list())\n",
    "    train_op_corpus = [op_dictionary.doc2bow(op) for op in train_ops.to_list()]\n",
    "    test_op_corpus = [op_dictionary.doc2bow(op) for op in test_ops.to_list()]\n",
    "    \n",
    "    # BEGIN VALIDATION. THIS WILL TAKE SOME TIME.\n",
    "\n",
    "    train_perplexity = []\n",
    "    test_perplexity = []\n",
    "    train_coherence = []\n",
    "    test_coherence = []\n",
    "    min_cite_dist_mean = []\n",
    "    min_cite_dist_sd = []\n",
    "    avg_cite_dist_mean = []\n",
    "    avg_cite_dist_sd = []\n",
    "    max_cite_dist_mean = []\n",
    "    max_cite_dist_sd = []\n",
    "\n",
    "    test_ids = data.loc[split:, 'case_id'].to_list()\n",
    "    data = None\n",
    "\n",
    "    wordcloud_header = os.path.join(wordcloud_header_top, dataset)\n",
    "    os.mkdir(wordcloud_header)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for nt in num_topics:\n",
    "        iter_start=time.time()\n",
    "        print(f'Processing model with {nt} topics...')\n",
    "        temp_time= time.time()\n",
    "        lda = LdaModel(train_op_corpus, id2word=op_dictionary, num_topics=nt)\n",
    "        print(f'Model training done. Time: {round(time.time()-temp_time)}')\n",
    "        print('Computing perplexity on train set.')\n",
    "        temp_time= time.time()\n",
    "        train_perplexity.append(get_perplexity(lda, train_op_corpus))\n",
    "        print(f'Done. Time: {round(time.time()-temp_time)}')\n",
    "        print('Computing perplexity on test set.')\n",
    "        temp_time= time.time()\n",
    "        test_perplexity.append(get_perplexity(lda, test_op_corpus))\n",
    "        print(f'Done. Time: {round(time.time()-temp_time)}')\n",
    "        print('Computing coherence on train set.')\n",
    "        temp_time= time.time()\n",
    "        train_coherence.append(get_coherence(lda, train_ops.to_list(), op_dictionary))\n",
    "        print(f'Done. Time: {round(time.time()-temp_time)}')\n",
    "        print('Computing coherence on test set.')\n",
    "        temp_time= time.time()\n",
    "        test_coherence.append(get_coherence(lda, test_ops.to_list(), op_dictionary))\n",
    "        print(f'Done. Time: {round(time.time()-temp_time)}')\n",
    "        print('Computing citation graph validation metrics.')\n",
    "        temp_time= time.time()\n",
    "        metric_cols = ['min_cite_dist', 'mean_cite_dist', 'max_cite_dist']\n",
    "        citation_dist_results = knn_citation_validation(test_ids, lda, test_op_corpus, big_subgraph, 5)[metric_cols]\n",
    "        min_cite_dist_mean.append(citation_dist_results.min_cite_dist.mean())\n",
    "        min_cite_dist_sd.append(citation_dist_results.min_cite_dist.std())\n",
    "        avg_cite_dist_mean.append(citation_dist_results.mean_cite_dist.mean())\n",
    "        avg_cite_dist_sd.append(citation_dist_results.mean_cite_dist.std())\n",
    "        max_cite_dist_mean.append(citation_dist_results.max_cite_dist.mean())\n",
    "        max_cite_dist_sd.append(citation_dist_results.max_cite_dist.std())\n",
    "        print(f'Done. Time: {round(time.time()-temp_time)}')\n",
    "        print('Saving wordclouds...')\n",
    "        temp_time= time.time()\n",
    "        os.mkdir(os.path.join(wordcloud_header, f'num_topics_{nt}'))\n",
    "        get_wordclouds(lda, save_file=os.path.join(wordcloud_header, f'num_topics_{nt}'), num_topics=nt)\n",
    "        print(f'Done. Time: {round(time.time()-temp_time)}')\n",
    "        print(f'Done with full iteration. TOTAL TIME: {round(time.time()-iter_start)}')\n",
    "        print('######################################')\n",
    "    print(f'FINISHED. TOTAL TIME ELAPSED: {time.time()-start}')\n",
    "\n",
    "    results_df = pd.DataFrame({'num_topics': num_topics,\n",
    "                              'train_perplexity': train_perplexity,\n",
    "                              'test_perplexity': test_perplexity,\n",
    "                              'train_coherence': train_coherence,\n",
    "                              'test_coherence': test_coherence,\n",
    "                              'min_cite_dist_mean': min_cite_dist_mean,\n",
    "                              'min_cite_dist_sd': min_cite_dist_sd,\n",
    "                              'avg_cite_dist_mean': avg_cite_dist_mean,\n",
    "                              'avg_cite_dist_sd': avg_cite_dist_sd,\n",
    "                              'max_cite_dist_mean': max_cite_dist_mean,\n",
    "                              'max_cite_dist_sd': max_cite_dist_sd})\n",
    "\n",
    "    results_df.to_csv(os.path.join(output_header, dataset + '.csv'), index=False)\n",
    "\n",
    "    train_ops = None\n",
    "    train_op_corpus = None\n",
    "    test_ops = None\n",
    "    test_op_corpus = None\n",
    "    op_dictionary = None\n",
    "    test_ids = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
