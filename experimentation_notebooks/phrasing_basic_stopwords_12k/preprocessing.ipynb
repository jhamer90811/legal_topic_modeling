{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook implements the preprocessing routine which applies bigram-phrasing to the tokens \n",
    "extracted by the baseline preprocessing routine as well as removal of basic legal stopwords.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import spacy\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "raw_data_header = '/Users/jhamer90811/Documents/Insight/legal_topic_modeling/data_uncompressed/raw_data_12k'\n",
    "\n",
    "# Note: due to time constraints, I am only preprocessing the 'cases_IL_after1950_12k' dataset\n",
    "\n",
    "# datasets = ['random_cases2', 'cases_after1950_12k', 'cases_IL_12k', 'cases_IL_after1950_12k']\n",
    "\n",
    "datasets = ['cases_IL_after1950_12k']\n",
    "\n",
    "baseline_data_header = '/Users/jhamer90811/Documents/Insight/legal_topic_modeling/data_uncompressed/baseline_12k'\n",
    "\n",
    "processed_data_header = '/Users/jhamer90811/Documents/Insight/legal_topic_modeling/data_uncompressed/phrasing_basic_stopwords_12k'\n",
    "\n",
    "def parse_list_col(df, col_to_parse):\n",
    "    df.loc[:, col_to_parse] = df[col_to_parse].apply(lambda x: x.strip('[]').split(','))\n",
    "    df.loc[:, col_to_parse] = df[col_to_parse].apply(lambda x: [t.strip().strip(\"'\") for t in x])\n",
    "    \n",
    "basic_legal_stopwords = {'a.',\n",
    " 'a.2d',\n",
    " 'a.3d',\n",
    " 'appeal',\n",
    " 'appellant',\n",
    " 'appellee',\n",
    " 'case',\n",
    " 'cir',\n",
    " 'court',\n",
    " 'defendant',\n",
    " 'f. supp.',\n",
    " 'f.supp.',\n",
    " 'f.supp.2d',\n",
    " 'f.supp.3d',\n",
    " 'fact',\n",
    " 'find',\n",
    " 'hold',\n",
    " 'judgment',\n",
    " 'n.e.',\n",
    " 'n.e.2d',\n",
    " 'opinion',\n",
    " 'order',\n",
    " 'p.',\n",
    " 'p.2d',\n",
    " 'p.3d',\n",
    " 'plaintiff',\n",
    " 'question',\n",
    " 's.e.',\n",
    " 's.e.2d',\n",
    " 's.e.3d',\n",
    " 's.w.',\n",
    " 's.w.2d',\n",
    " 's.w.3d.',\n",
    " 'see',\n",
    " 'so.',\n",
    " 'so.2d',\n",
    " 'state',\n",
    " 'time',\n",
    " 'trial'}\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training bigram phraser...\n",
      "Done.\n",
      "Beginning preprocessing of opinions for cases_IL_after1950_12k...\n",
      "Done with 1000 opinions.\n",
      "Time elapsed: 236\n",
      "#######################################\n",
      "Done with 2000 opinions.\n",
      "Time elapsed: 479\n",
      "#######################################\n",
      "Done with 3000 opinions.\n",
      "Time elapsed: 719\n",
      "#######################################\n",
      "Done with 4000 opinions.\n",
      "Time elapsed: 955\n",
      "#######################################\n",
      "Done with 5000 opinions.\n",
      "Time elapsed: 1198\n",
      "#######################################\n",
      "Done with 6000 opinions.\n",
      "Time elapsed: 1445\n",
      "#######################################\n",
      "Done with 7000 opinions.\n",
      "Time elapsed: 1686\n",
      "#######################################\n",
      "Done with 8000 opinions.\n",
      "Time elapsed: 1934\n",
      "#######################################\n",
      "Done with 9000 opinions.\n",
      "Time elapsed: 2172\n",
      "#######################################\n",
      "Done with 10000 opinions.\n",
      "Time elapsed: 2414\n",
      "#######################################\n",
      "Done with 11000 opinions.\n",
      "Time elapsed: 2647\n",
      "#######################################\n",
      "Done with 12000 opinions.\n",
      "Time elapsed: 2828\n",
      "#######################################\n",
      "Done preprocessing. Took 2828 seconds.\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "for w in basic_legal_stopwords:\n",
    "    nlp.vocab[w].is_stop = True\n",
    "\n",
    "for dataset in datasets:\n",
    "    baseline_data = pd.read_csv(os.path.join(baseline_data_header, dataset + '_processed.csv'))\n",
    "    parse_list_col(baseline_data, 'opinion')\n",
    "    print(f'Training bigram phraser...')\n",
    "    bigram = Phrases(baseline_data['opinion'].to_list(), min_count=5, threshold=100)\n",
    "    bigram_mod = Phraser(bigram)\n",
    "    print('Done.')\n",
    "    baseline_data = None\n",
    "    \n",
    "    data_raw = pd.read_csv(os.path.join(raw_data_header, dataset + '_raw.csv'))\n",
    "    start = time.time()\n",
    "    opinions = []\n",
    "    print(f'Beginning preprocessing of opinions for {dataset}...')\n",
    "    for i, row in data_raw.iterrows():\n",
    "        case_id = row.case_id\n",
    "        juris = row.jurisdiction\n",
    "        text = row.opinion\n",
    "        court = row.court_name\n",
    "        if dataset=='random_cases2':\n",
    "            decision_year = pd.to_datetime(row.decision_date).year\n",
    "        else:\n",
    "            decision_year = row.decision_year\n",
    "        doc = nlp(text)\n",
    "        doc = [t.lemma_ for t in doc if (not t.is_stop) and\\\n",
    "                                        (t.lemma_ not in basic_legal_stopwords) and\\\n",
    "                                        (t.pos_ in ['NOUN', 'ADJ', 'VERB', 'ADV'])]\n",
    "        doc = bigram_mod[doc]\n",
    "        opinions.append((case_id, juris, court, decision_year, doc))\n",
    "        if (i+1)%1000==0:\n",
    "            print(f'Done with {i+1} opinions.')\n",
    "            print(f'Time elapsed: {round(time.time()-start)}')\n",
    "            print('#######################################')\n",
    "    print(f'Done preprocessing. Took {round(time.time()-start)} seconds.')\n",
    "\n",
    "    data_raw = None\n",
    "\n",
    "    df = pd.DataFrame(opinions, columns=['case_id', 'jurisdiction', 'court', 'decision_year', 'opinion'])\n",
    "    opinions = None\n",
    "    df.to_csv(os.path.join(processed_data_header, dataset + '_processed.csv'), index=False)\n",
    "\n",
    "    df = None\n",
    "    \n",
    "nlp = None  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
